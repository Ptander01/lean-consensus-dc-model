"""
Export gold feature class schemas to version-controllable formats
Exports to: JSON, Markdown, CSV, and XML
"""

import arcpy
import json
import os
from datetime import datetime

# Configuration
GDB_PATH = r"C:\Users\ptanderson\Documents\ArcGIS\Projects\Lean Consensus DC Model\Default.gdb"
OUTPUT_DIR = r"C:\Users\ptanderson\Documents\ArcGIS\Projects\Lean Consensus DC Model\schema_exports"
FEATURE_CLASSES = ["gold_buildings", "gold_campus"]

# Create output directory if it doesn't exist
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"Created output directory: {OUTPUT_DIR}")

# Export timestamp
export_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def get_field_info(fc_path):
    """Extract detailed field information from feature class"""
    fields_info = []
    
    for field in arcpy.ListFields(fc_path):
        field_dict = {
            "name": field.name,
            "type": field.type,
            "length": field.length if field.length else None,
            "precision": field.precision if hasattr(field, 'precision') else None,
            "scale": field.scale if hasattr(field, 'scale') else None,
            "alias": field.aliasName,
            "nullable": field.isNullable,
            "required": field.required,
            "editable": field.editable,
            "domain": field.domain if field.domain else None
        }
        fields_info.append(field_dict)
    
    return fields_info

def get_spatial_reference(fc_path):
    """Get spatial reference information"""
    desc = arcpy.Describe(fc_path)
    sr = desc.spatialReference
    
    return {
        "name": sr.name,
        "type": sr.type,
        "factory_code": sr.factoryCode,
        "wkid": sr.factoryCode,
        "wkt": sr.exportToString()
    }

def get_record_count(fc_path):
    """Get record count"""
    return int(arcpy.GetCount_management(fc_path)[0])

def export_to_json(fc_name, fc_path):
    """Export schema to JSON format"""
    desc = arcpy.Describe(fc_path)
    
    schema = {
        "feature_class": fc_name,
        "export_date": export_time,
        "geometry_type": desc.shapeType,
        "record_count": get_record_count(fc_path),
        "spatial_reference": get_spatial_reference(fc_path),
        "fields": get_field_info(fc_path),
        "has_m": desc.hasM,
        "has_z": desc.hasZ
    }
    
    output_file = os.path.join(OUTPUT_DIR, f"{fc_name}_schema.json")
    with open(output_file, 'w') as f:
        json.dump(schema, f, indent=2)
    
    print(f"✓ Exported JSON: {output_file}")
    return output_file

def export_to_markdown(fc_name, fc_path):
    """Export schema to Markdown table format"""
    fields = get_field_info(fc_path)
    desc = arcpy.Describe(fc_path)
    sr = get_spatial_reference(fc_path)
    record_count = get_record_count(fc_path)
    
    md_content = f"""# {fc_name} Schema

**Exported:** {export_time}  
**Geometry Type:** {desc.shapeType}  
**Spatial Reference:** {sr['name']} (WKID: {sr['wkid']})  
**Record Count:** {record_count:,}

---

## Fields ({len(fields)} total)

| Field Name | Type | Length | Precision | Scale | Nullable | Alias | Domain |
|------------|------|--------|-----------|-------|----------|-------|--------|
"""
    
    for field in fields:
        length = field['length'] if field['length'] else '-'
        precision = field['precision'] if field['precision'] else '-'
        scale = field['scale'] if field['scale'] else '-'
        nullable = 'Yes' if field['nullable'] else 'No'
        alias = field['alias'] if field['alias'] != field['name'] else '-'
        domain = field['domain'] if field['domain'] else '-'
        
        md_content += f"| `{field['name']}` | {field['type']} | {length} | {precision} | {scale} | {nullable} | {alias} | {domain} |\n"
    
    # Add spatial reference details
    md_content += f"""
---

## Spatial Reference Details

**Name:** {sr['name']}  
**Type:** {sr['type']}  
**WKID:** {sr['wkid']}

### Well-Known Text (WKT)

"""
    # Add WKT in code block (avoiding triple-quote conflict)
    md_content += "```\n"
    md_content += sr['wkt']
    md_content += "\n```\n"
    
    md_content += """
---

## Field Type Summary

"""
    
    # Count field types
    type_counts = {}
    for field in fields:
        ftype = field['type']
        type_counts[ftype] = type_counts.get(ftype, 0) + 1
    
    for ftype, count in sorted(type_counts.items()):
        md_content += f"- **{ftype}:** {count} fields\n"
    
    md_content += f"\n---\n\n*Generated by export_schema.py on {export_time}*\n"
    
    output_file = os.path.join(OUTPUT_DIR, f"{fc_name}_schema.md")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(md_content)
    
    print(f"✓ Exported Markdown: {output_file}")
    return output_file

def export_to_csv(fc_name, fc_path):
    """Export field definitions to CSV"""
    import csv
    
    fields = get_field_info(fc_path)
    
    output_file = os.path.join(OUTPUT_DIR, f"{fc_name}_fields.csv")
    
    with open(output_file, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # Header
        writer.writerow([
            'Field Name', 'Type', 'Length', 'Precision', 'Scale', 
            'Nullable', 'Required', 'Editable', 'Alias', 'Domain'
        ])
        
        # Data rows
        for field in fields:
            writer.writerow([
                field['name'],
                field['type'],
                field['length'] if field['length'] else '',
                field['precision'] if field['precision'] else '',
                field['scale'] if field['scale'] else '',
                'Yes' if field['nullable'] else 'No',
                'Yes' if field['required'] else 'No',
                'Yes' if field['editable'] else 'No',
                field['alias'],
                field['domain'] if field['domain'] else ''
            ])
    
    print(f"✓ Exported CSV: {output_file}")
    return output_file

def export_to_xml_workspace(fc_name, fc_path):
    """Export to ArcGIS XML Workspace Document (schema only)"""
    output_file = os.path.join(OUTPUT_DIR, f"{fc_name}_workspace.xml")
    
    try:
        arcpy.ExportXMLWorkspaceDocument_management(
            in_data=fc_path,
            out_file=output_file,
            export_type="SCHEMA_ONLY",
            storage_type="NORMALIZED"
        )
        print(f"✓ Exported XML Workspace: {output_file}")
        return output_file
    except Exception as e:
        print(f"✗ Error exporting XML: {e}")
        return None

def export_field_domains():
    """Export any domains defined in the geodatabase"""
    domains = arcpy.da.ListDomains(GDB_PATH)
    
    if not domains:
        print("No domains found in geodatabase")
        return None
    
    domain_info = []
    for domain in domains:
        domain_dict = {
            "name": domain.name,
            "description": domain.description,
            "type": domain.domainType,
            "field_type": domain.type,
            "split_policy": domain.splitPolicy,
            "merge_policy": domain.mergePolicy
        }
        
        # Get coded values if it's a coded value domain
        if domain.domainType == 'CodedValue':
            domain_dict["coded_values"] = domain.codedValues
        elif domain.domainType == 'Range':
            domain_dict["range"] = [domain.range[0], domain.range[1]]
        
        domain_info.append(domain_dict)
    
    # Export to JSON
    output_file = os.path.join(OUTPUT_DIR, "geodatabase_domains.json")
    with open(output_file, 'w') as f:
        json.dump(domain_info, f, indent=2)
    
    print(f"✓ Exported Domains: {output_file}")
    return output_file

def create_summary_readme():
    """Create a README for the schema exports"""
    readme_content = f"""# Schema Exports

Schema documentation for the AI Data Center Consensus GIS project.

**Exported:** {export_time}  
**Source Geodatabase:** `{GDB_PATH}`

---

## Exported Files

### gold_buildings
- `gold_buildings_schema.json` - Complete schema in JSON format
- `gold_buildings_schema.md` - Human-readable Markdown documentation
- `gold_buildings_fields.csv` - Field list in CSV format
- `gold_buildings_workspace.xml` - ArcGIS XML Workspace Document

### gold_campus
- `gold_campus_schema.json` - Complete schema in JSON format
- `gold_campus_schema.md` - Human-readable Markdown documentation
- `gold_campus_fields.csv` - Field list in CSV format
- `gold_campus_workspace.xml` - ArcGIS XML Workspace Document

### Geodatabase
- `geodatabase_domains.json` - Domain definitions (if any)

---

## File Formats

### JSON (.json)
- **Purpose:** Machine-readable, version-controllable
- **Use Case:** Schema validation, automated testing, API documentation
- **Tools:** Can be parsed by Python, JavaScript, etc.

### Markdown (.md)
- **Purpose:** Human-readable documentation
- **Use Case:** GitHub display, documentation websites
- **Tools:** Renders nicely on GitHub, can be converted to HTML/PDF

### CSV (.csv)
- **Purpose:** Simple tabular format
- **Use Case:** Excel analysis, quick field reference
- **Tools:** Opens in Excel, Google Sheets, text editors

### XML (.xml)
- **Purpose:** ArcGIS native format
- **Use Case:** Reimporting schema to new geodatabase
- **Tools:** ArcGIS Pro, ArcCatalog

---

## Updating Schemas

To regenerate these schema exports after making changes:

"""
    
    # Add code block separately (avoiding triple-quote conflict)
    readme_content += "```python\n"
    readme_content += "python scripts/export_schema.py\n"
    readme_content += "```\n"
    
    readme_content += """
---

## Version Control

These files are designed for Git version control:
- ✅ Text-based formats (JSON, MD, CSV, XML)
- ✅ Readable diffs show field changes
- ✅ Small file sizes
- ✅ No binary geodatabase files

---

*Generated by export_schema.py*
"""
    
    output_file = os.path.join(OUTPUT_DIR, "README.md")
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(readme_content)
    
    print(f"✓ Created README: {output_file}")
    return output_file

# ============================================================
# MAIN EXECUTION
# ============================================================

if __name__ == "__main__":
    print("=" * 60)
    print("SCHEMA EXPORT SCRIPT")
    print("=" * 60)
    print(f"Geodatabase: {GDB_PATH}")
    print(f"Output Directory: {OUTPUT_DIR}")
    print(f"Export Time: {export_time}")
    print("=" * 60)
    print()
    
    # Check if geodatabase exists
    if not arcpy.Exists(GDB_PATH):
        print(f"ERROR: Geodatabase not found at {GDB_PATH}")
        exit(1)
    
    # Export each feature class
    for fc_name in FEATURE_CLASSES:
        fc_path = os.path.join(GDB_PATH, fc_name)
        
        if not arcpy.Exists(fc_path):
            print(f"WARNING: Feature class '{fc_name}' not found, skipping...")
            continue
        
        print(f"\nExporting schema for: {fc_name}")
        print("-" * 60)
        
        # Export to all formats
        export_to_json(fc_name, fc_path)
        export_to_markdown(fc_name, fc_path)
        export_to_csv(fc_name, fc_path)
        export_to_xml_workspace(fc_name, fc_path)
        
        print()
    
    # Export domains
    print("Exporting geodatabase domains...")
    print("-" * 60)
    export_field_domains()
    print()
    
    # Create summary README
    print("Creating summary documentation...")
    print("-" * 60)
    create_summary_readme()
    
    print()
    print("=" * 60)
    print("EXPORT COMPLETE!")
    print("=" * 60)
    print(f"\nAll schema files saved to: {OUTPUT_DIR}")
    print("\nNext steps:")
    print("1. Review the exported files")
    print("2. Copy schema_exports/ folder to your GitHub repo")
    print("3. Add and commit to Git")
    print("4. Push to GitHub")
    print()